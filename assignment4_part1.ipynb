{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.1: Sarcasm Detection\n",
    "\n",
    "Way back in Exercise 1.1, you trained a text classifier for an NLP problem that was mostly objective: _nationality classification._ You likely attained pretty high performance too! In that task, there was ample evidence of the class in the data itself that made it easier to determine the right class. In the worst case, you could even look at the text and likely figure out what the class was for yourself. \n",
    "\n",
    "Here in Exercise 4.1, we'll be working with a harder task: _sarcasm detection._ Sarcasm is one of the many (*many*) kinds of social information that people regularly encode in text, such as respect, solidarity, intimacy, politeness, humor, etc. Despite their prevalence in communication, recognizing these signals is often far more challenging than tasks like you did in Week 1.\n",
    "\n",
    "As a task, sarcasm detection is surprisingly useful in downstream applications. For example, if trying to estimate public opinion or the sentiment around a business/product, the model should likely exclude sarcastic tweets from its estimate.\n",
    " \n",
    "This week's exercises will have you work with sarcasm labels from [SARC 2.0](https://nlp.cs.princeton.edu/SARC/2.0/main/), a large, real-world Reddit corpus that users labeled with sarcasm via the `/s` at the end of their message to indicate a reply is intended to be read as sarcastic. This `/s` is a great example of how humans try to provide explicit cues to others on how to read their message when other extra-textual signals like pitch or gesture (e.g. eye rolling) are unavaiable. We've cleaned up the data some and will be working with a **balanced** corpus for training where you're given equal numbers of sarcastic and non-sarcastic messages. However, we'll try testing in two setups: (1) a balanced setting that matches the real-world and (2) an unbalanced setting that mirrors the expected real-world distribution of sarcasm (after all, 50% of messages aren't sarcastic). \n",
    "\n",
    "This assignment has two learning goals.\n",
    "1. The first is to help you build a basic text classifier for this hard task and to then given some initial starting points to see whether you can improve upon this score. As a challenging and natural task, there are many avenues for improvement and this setting makes for a fun way to explore the data to see if you can identify opportunities to improve.\n",
    "2. The second learning goal has you use an Interpetable Machine Learning framework to explain what your classifier has learned. Here, we'll use the [LIME](https://github.com/marcotcr/lime/) python library which aims to build simple rules to explain the predictions of any trained classifier. LIME is particularly useful for getting a sense of what a classifier is \"looking at\" when making its decision and using this knowledge to add or remove features that seem useful.\n",
    "\n",
    "**Note:** As this corpus is web-gathered and made by humans, there's likely messages in it that you might find offensive. You are not requried to engage in the content as a part of this exercise, nor do the instructors condone any of the offensive statements that are potentially within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import bz2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 655"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the comment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('assets/sarcasm.train.tsv.gz', sep='\\t', compression='gzip').dropna()\n",
    "test_imb_df = pd.read_csv('assets/sarcasm.test-imb.tsv.gz', sep='\\t', compression='gzip').dropna()\n",
    "test_bal_df = pd.read_csv('assets/sarcasm.test-bal.tsv.gz', sep='\\t', compression='gzip').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1.1:  Convert the training/test corpus to TFIDF feature vectors\n",
    "Create a `TfidfVectorizer` that we'll use to featurize the corpus. For efficiency, we'll use a min document frequency of `100`, add English stop words, and use unigrams and bigrams in our vectorizer. You should fit the vectorizer on the trainin data and call this `X_train`. Then transform the balanced and imbalanced data, referring to these as `X_test_bal` and `X_test_imb` (we'll use the `imb` and `bal` notation throughout this notebook).\n",
    "\n",
    "Once you've finished vectorizing, we'll also want lists of the target lables for each, which we'll call `y_train`, `y_test_bal`, and `y_test_imb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "967e8cb1793d22f235abc0cffd3efa80",
     "grade": false,
     "grade_id": "cell-ca20f73175117649",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1.2:  Check the shape of the training/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "875c085affcb32b4c887bc54e98b4462",
     "grade": true,
     "grade_id": "cell-cbc3d0515ab672e9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1.3:  Sanity check the label count for the training data\n",
    "As a sanity check (which is a good habit), make a `Counter` for the labels in `y_train`. We expect this dataset to be balanced, so we should see a near-equal number of instances of each. Note that in this dataset, sarcasm is labeled as `1` and non-sarcastic as `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "696b0f99c767d753bc74e745ae12d11f",
     "grade": false,
     "grade_id": "cell-2c769baaa3c7e29d",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "578e8f8754ef351c24c2495fcd0323be",
     "grade": true,
     "grade_id": "cell-b3a8c8898833046e",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1.4:  Train our model\n",
    "Initialize a `LogisticRegression` classifier that we'll refer to as `lr_clf`.  You should set the solver to `lbfgs` and use `auto` for multi_class. \n",
    "\n",
    "For comparison, let's also initialize a `RandomForestClassifier` and call that `rf_clf`. Random Forests are powerful classifiers that let us use conjunctive features. However their tree structure can make them much slower to train. Let add some arguments to set the number of decision tree estimators to `50` and restrict their depth to at most `15`. This will likely make the trees less accurate as classifiers, but hopefully good enough for comparison. Be sure to set the `random_state` to the official seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68220e85659ed56c047b4152ebc109ad",
     "grade": false,
     "grade_id": "cell-af1faf5a8a9c4aba",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('started fitting')\n",
    "lr_clf.fit(X_train, y_train)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "print('finished fitting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1.5:  Create a dummy baseline\n",
    "\n",
    "As always, we'll create a `DummyClassifier` to use as a baseline for performance comparison. Since our data is balanced, we are okay to just use a dummy  classifier that randomly guesses the output class with uniform probability (there's no \"most-frequent class\" in balanced data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73fe65b455cf13e7562b57ea8b40991d",
     "grade": false,
     "grade_id": "cell-17519031858e0216",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate our model\n",
    "Let's see how well each of the models does on each of the test sets. Note that they have different distributions of data so we might see suprisingly different performance! We've provided the coded to save you some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bal = lr_clf.predict(X_test_bal)\n",
    "y_pred_imb = lr_clf.predict(X_test_imb)\n",
    "rf_y_pred_bal = rf_clf.predict(X_test_bal)\n",
    "rf_y_pred_imb = rf_clf.predict(X_test_imb)\n",
    "random_y_pred_bal = random_clf.predict(X_test_bal)\n",
    "random_y_pred_imb = random_clf.predict(X_test_imb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1.6:  Score the predictions\n",
    "Let's evaluate our model. Since we only have two classes and we care about the predictions for sarcasm, it's  appropriate to use a binary F1 score here. We'll generate  F1 scores for each of the three models on both the balanced and imbalanced dataset using the naming scheme `lr_imb_f1`, `rf_imb_f1`, `rand_imb_f1`, `lr_bal_f1`, `rf_bal_f1`, `rand_bal_f1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0af4eabe9475304a424a840aa2e7bfd",
     "grade": false,
     "grade_id": "cell-ed41c2c8f239c6a5",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4cc177e1d11b19b9f186d68c215cae90",
     "grade": true,
     "grade_id": "cell-a2bcdf2331d8226f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining predictions using lime\n",
    "In part 2 of the notebook, let's use Lime to explain what the models are picking up on! We'll be using the `make_pipeline` feature of Sklearn that lets us chain our vectorizer and classifier together into a common classifier-like object that we can pass to Lime. Here, we'll look at what the `LogisticRegression` and `RandomForest` models are using as important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1.7:  Set up an explainer and classifier pipelines\n",
    "Create a new [LimeTextExplainer](https://lime-ml.readthedocs.io/en/latest/lime.html#lime.lime_text.LimeTextExplainer) that we'll use to examine predictions. One useful argument to the class is `class_names` which let you specify what the classes are called, which we recommend you set with human-readable names (e.g., sarcastic and not-sarcastic).\n",
    "\n",
    "Additionally, create two classifier pipelines using the `make_pipeline` call, one for the trained `RandomForestClassifier` and one for the `LogisticRegression`. Both pipelines should take in the already-trained vectorizer as their first stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91b26a2915b47212b271107a0cf7f07d",
     "grade": false,
     "grade_id": "cell-9a2acbf79f945af8",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick a particular instance and see what features the model is using to decide its class. Here, we'll use the class seen and sample one item from the imbalanced dataset. In your own exploration, feel free to sample different items and see how the predictions change!\n",
    "\n",
    "Note that both classifiers use a decision boundary of 0.5, so values above this will be considered 1 (sarcastic) whereas values below will be considered 0 (not sarcastic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_row_df = test_imb_df.sample(1, random_state=RANDOM_SEED)\n",
    "_, inst_text, label = next(test_row_df.itertuples())\n",
    "\n",
    "print('Comment has text \"%s\"' %(inst_text))\n",
    "# print('True label: %d' % label)\n",
    "print('LogisticRegression Probability(Sarcastic) =', lr_pipe.predict_proba([inst_text])[0, 1])\n",
    "print('RandomForest Probability(Sarcastic) =', rf_pipe.predict_proba([inst_text])[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the models disagree on this instance! What do you think the label should be? (Uncomment the true label line to find out, but try to guess first!) Let's use Lime to see what the classifiers are using as features in making their predictions.\n",
    "\n",
    "### Task 4.1.8:  Generate two explanations\n",
    "Use the `explainer` object to generate predictions for each of the classifier pipelines, passing in `inst_text` as the item to be classified. Each explanation should report 10 features. Call these explanations `rf_explanation` and `lr_explanation`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e26bc499b26965d136fe855000721651",
     "grade": false,
     "grade_id": "cell-0c96ca197176bd35",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "587802a7d136d07e2ab7ac5d617e3ad8",
     "grade": true,
     "grade_id": "cell-37d240becd87044e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the explanations\n",
    "Let's take a look at what Lime estimates to be the important features and how much weight the model is placing on each. As a first pass we can print out what features and their estimated weights for each. Note that features that indicate a more sarcastic comment will be scored **positive** and features for non-sarcastic comments are **negative**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat, val in lr_explanation.as_list():\n",
    "    print('LogisticRegression has an estimated weight of %f on feature \"%s\"' % (val, feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lime also makes it easy to visualizize these weights as well! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = lr_explanation.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = rf_explanation.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the LogisticRegression model is finding multiple features. Both models seem to be picking up on \"just\" as a feature, which likely reflects its usage in trivializing/minimizing language, e.g., \"it's _just_ a homework assignment,\" and probably correlates with sarcastic languauge (see [Kiesling (2011)](https://www.researchgate.net/profile/Scott_Kiesling2/publication/277710719_Stance_in_Context_Affect_alignment_and_investment_in_the_analysis_of_stancetaking/links/55708f0308aeec5e6fd263b0/Stance-in-Context-Affect-alignment-and-investment-in-the-analysis-of-stancetaking.pdf) if you want to learn more on the sociolinguistic uses of \"just\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1.9:  Look at model confidence scores\n",
    "Both models were not very confident in  their predictions (scores near the decision boundary of 0.5). As one final diagnosis, let's look at distribution of model confidences across the instances.\n",
    "\n",
    "Create two lists, `rf_imb_probs` and `lr_imb_probs` to hold the probability values of each model's predictions on the imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc95a7dedaa4741e16df629d9aa8fe0a",
     "grade": false,
     "grade_id": "cell-3fc91d20645c42f2",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put these probabilities into a data frame so we can plot them cleanly with Seaborn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = { 'probability': [], 'model': []}\n",
    "df['probability'].extend(rf_imb_probs)\n",
    "df['model'].extend(['RandomForest'] * len(rf_imb_probs))\n",
    "df['probability'].extend(lr_imb_probs)\n",
    "df['model'].extend(['LogisticRegression'] * len(lr_imb_probs))\n",
    "df = pd.DataFrame(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a kde plot to make some nice looking curves to visualize the two models' distributions over confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(\n",
    "   data=df, x=\"probability\", hue=\"model\", \n",
    "   fill=True, common_norm=False,\n",
    "   alpha=.5, linewidth=0, bw_adjust=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! The Logistic Regression model is slightly biased towards labeling item as sarcastic (note its mean is above 0.5), which is problematic in our imbalanced setting where sarcasm is much more rare. In contrast, the Random Forest model has its probabiliites centered closer to 0.5, suggesting it's never quite confident. If you see this in the real world, it could be a sign that you need to rebalance your training data so that models are biased towards the expected distribution of the classes.\n",
    "\n",
    "# Next steps\n",
    "This exercise has shown you another text classification problem for estimating whether a message is sarcastic using a massive real-world dataset. When using the simple models from before, the performance is much lower than when we tried to predict nationality! To dig into this a bit more, we've look at two classifiers, used Lime to explain what each is looking at, and examined their prediction distributions. These point to a few ways you can try exploring the data and models in next steps:\n",
    "\n",
    "* Identify data points that the models are confident about and see if you agree with the features the model is looking at. How would you adjust the model accordingly?\n",
    "* Try adjusting the hyperparameters of the models to see if you can improve performance. We've set the current values to ensure the script runs efficiently in the auto-grader, but you can increase the values in your own testing. The RandomForestClassifier has many potential options (e.g., using more trees, allowing for deeper trees) that may give you a more robust classifier.\n",
    "* In Exercise 2.2, we used dense representations; given the short tests here, dense representations might be useful for learning across models.\n",
    "* There are some words that seem more sarcastic than others. One idea is to create a lexicon of sarcastic words and add these as features.\n",
    "* The training data is balanced, which is often useful for training a classifier. However, since we might way to apply this in a real setting where sarcasm is rare, one idea is to _rebalance_ the training data to remove some instance (also known as undersampling) to improve performance.\n",
    "* Another option instead of undersampling is to tell the model to penalize some mistakes more than others. Sklearn supports this with its `class_weight` argument. You could try to increase the class weight to make the model more or less conservative for labeling an instance as sarcastic."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_applied_natural_language_processing_v2_assignment4_part1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
