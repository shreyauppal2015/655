{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.1 - Train a Nationality Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started in NLP\n",
    "\n",
    "In this first exercise, you'll begin your NLP journey by training a text classifier and exploring what kinds of options affect its performance. Text classification problems are everywhere NLP and here, we'll be looking at Wikipedia articles, which feature a lot of very interesting meta-data we can use to reasonable its text. Specifically, in the first exercise, we'll be looking at biographies of people and trying to predict their nationality, as reported in Wikipedia. Nationalities make for an interesting class to predict since biographies often contain many descriptions of places, events, organizations (e.g., universities), that all help ground a person in a geographic area. However, people are often international and move about&mdash;perhaps some of you have also moved about too&mdash;so the task is not trivial!\n",
    "\n",
    "As a part of this first notebook, we'll build a simple classifier, `LogisticRegression` using the [sklearn](https://scikit-learn.org/stable/index.html) package. This package is often the initial toolbox for NLP practitioners to build and prototype models. Many of its classes provide useful built-in functionality for doing NLP preprocessing steps like tokenization, counting bigrams, or calculating TF-IDF values. These steps simplify much of what you need to build to create a \"minimum viable product\" in your first classifier (for any task!) and get a sense of how challenging the task is. We opt to start with Logistic Regression because it is not only quite quick to train, but performs competitively in many settings. However,  many `sklearn` classes for prediction have the same interface, so the skills you learn here will let you easily test out other classifiers and use them in practice.\n",
    "\n",
    "In one part of this notebook, you'll explore how much data you need. Wikipedia is big enough that we have lots of examples of many nationalities. This surplus of data can let us try estimating the effect of the amount of data on our eventual performance. \n",
    "\n",
    "Finally, as an NLP _practitioner_ you'll have many tools at your disposal. This notebook is just a start for options you could explore. We've listed a few ways you could try exploring on your own to learn more about text classification. Sometimes the best way to see how something works is to try it out yourself. If you finish all the notebooks and want to go even further and see what current NLP is up to, the field often hosts what are known as _shared tasks_ through the [SemEval](https://semeval.github.io/) workshop series, where anyone can try solving a current research problem on training data researchers have released (sort of like a research Kaggle). If you want to try your hand, feel try to try developing a method for a SemEval task&mdash;some of which might still be going on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f2301879781da8994f5d43d45ec5aeb",
     "grade": false,
     "grade_id": "cell-948e57860220ca98",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "It's good practice to manually set your random seed when performing machine learning experiments so that they are reproducible for others. Here, we set our seed to 655 to ensure your models and experiments get the expected results when evaluating your homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 655"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce7eace7b6705c89818e2ad0bea597e4",
     "grade": false,
     "grade_id": "cell-53351283c328354d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Data Processing\n",
    "\n",
    "Read in the corpus file, which is in JSON lines format (one line per JSON object). Each line represents cleaned up data of a single Wikipedia article for a person. For this exercise, we'll construct a list of tuples where the first element is the aggregated `bio` of the person's article (their biography) and the second element is their nationality. The birth year is specified as a string in the `nationality` field of the `infobox` of the page. If someone doesn't have a nationality in their infobox, you should skip their article.\n",
    "\n",
    "*Important Note:* Wikipedians sometimes have inconsistent spelling and capitalization for nationalities. To add some minimal text normalization you should _lower case_ the nationality. In general, it's always a good idea to look at how noisy your text is before deciding on its final form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nationality_df = pd.read_csv('assets/nationality.tsv.gz', sep='\\t', compression='gzip')\n",
    "nationality_df = nationality_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>bio</th>\n",
       "      <th>nationality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Alain Connes (born 1 April 1947) is a French m...</td>\n",
       "      <td>french</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Life\\n=== Early life ===\\nSchopenhauer's birth...</td>\n",
       "      <td>german</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Life and career\\nAlfred Nobel at a young age i...</td>\n",
       "      <td>swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Early life\\nAlfred Vogt (both \"Elton\" and \"van...</td>\n",
       "      <td>canadian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Alfons Maria Jakob (2 July 1884 in Aschaffenbu...</td>\n",
       "      <td>german</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                bio nationality\n",
       "0           0  Alain Connes (born 1 April 1947) is a French m...      french\n",
       "1           1  Life\\n=== Early life ===\\nSchopenhauer's birth...      german\n",
       "2           2  Life and career\\nAlfred Nobel at a young age i...     swedish\n",
       "3           3  Early life\\nAlfred Vogt (both \"Elton\" and \"van...    canadian\n",
       "4           4  Alfons Maria Jakob (2 July 1884 in Aschaffenbu...      german"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nationality_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca1e9dd8bdbe1a7746b50fa94fe4bfb3",
     "grade": false,
     "grade_id": "cell-0171ee647d8b4af8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Let's check that you have things loaded correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9939b1603b286212b09dd2c14a9b1410",
     "grade": false,
     "grade_id": "cell-a584653bc9655260",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 1.1.1: Print the dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d90c9193636647cfbebe107f4b5018b",
     "grade": true,
     "grade_id": "dataset_size",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319358\n"
     ]
    }
   ],
   "source": [
    "print(len(nationality_df)) # Should be 319358\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9236a3574cbf82a03d66271e227f5bbf",
     "grade": false,
     "grade_id": "cell-654d8884258fb0b7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 1.1.2: Print the number of nationality labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d48d3ce91c7f1bce3a222ec7a1e2a8c2",
     "grade": true,
     "grade_id": "num_nationalities_1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13616\n"
     ]
    }
   ],
   "source": [
    "print(len(set(nationality_df.nationality))) # Should be 13616\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1.3: Print out the top 100 most common nationalities to see what Wikipedia's labels look like\n",
    "You might have noticed that's a huge number of nationalities! What might be going on in the data? Let's use python's `Counter` object to print out the 100 most common nationalities in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c19583ca8b5c3265592cf9af62d72986",
     "grade": true,
     "grade_id": "most_common_nationalities_1",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('american', 43157),\n",
       " ('british', 16635),\n",
       " ('indian', 10761),\n",
       " ('united states, american', 9811),\n",
       " ('australian', 8285),\n",
       " ('french', 7365),\n",
       " ('german', 6628),\n",
       " ('italian', 5403),\n",
       " ('japanese', 5223),\n",
       " ('canadian', 5083),\n",
       " ('united states', 4915),\n",
       " ('americans, american', 3804),\n",
       " ('english', 3507),\n",
       " ('mexican', 3294),\n",
       " ('canadians, canadian', 3280),\n",
       " ('pakistani', 3114),\n",
       " ('spanish', 3090),\n",
       " ('usa', 2973),\n",
       " ('russian', 2910),\n",
       " ('united kingdom, british', 2894),\n",
       " ('norwegian', 2851),\n",
       " ('polish', 2790),\n",
       " ('chinese', 2456),\n",
       " ('irish', 2218),\n",
       " ('swedish', 2189),\n",
       " ('dutch', 2103),\n",
       " ('brazilian', 1936),\n",
       " ('austrian', 1812),\n",
       " ('south korean', 1762),\n",
       " ('nigerian', 1705),\n",
       " ('swiss', 1643),\n",
       " ('british people, british', 1621),\n",
       " ('bangladeshi', 1613),\n",
       " ('hungarian', 1599),\n",
       " ('belgian', 1597),\n",
       " ('germany, german', 1544),\n",
       " ('serbian', 1521),\n",
       " ('danish', 1454),\n",
       " ('turkish', 1433),\n",
       " ('south african', 1430),\n",
       " ('irish people, irish', 1418),\n",
       " ('romanian', 1389),\n",
       " ('france, french', 1376),\n",
       " ('argentine', 1362),\n",
       " ('eng', 1278),\n",
       " ('czech', 1262),\n",
       " ('scottish', 1159),\n",
       " ('indian people, indian', 1144),\n",
       " ('greek', 1142),\n",
       " ('finnish', 1129),\n",
       " ('sri lankan', 1101),\n",
       " ('filipino', 1070),\n",
       " ('italy, italian', 1068),\n",
       " ('aus', 1061),\n",
       " ('ghanaian', 1017),\n",
       " ('iranian', 1016),\n",
       " ('croatian', 1015),\n",
       " ('netherlands, dutch', 977),\n",
       " ('ugandan', 964),\n",
       " ('united kingdom', 954),\n",
       " ('egyptian', 942),\n",
       " ('bulgarian', 923),\n",
       " ('japanese people, japanese', 894),\n",
       " ('new zealand', 890),\n",
       " ('israeli', 855),\n",
       " ('india', 821),\n",
       " ('spain, spanish', 765),\n",
       " ('cuban', 765),\n",
       " ('ukrainian', 742),\n",
       " ('republic of china', 712),\n",
       " ('kenyan', 679),\n",
       " ('french people, french', 664),\n",
       " ('chilean', 637),\n",
       " ('english people, english', 633),\n",
       " ('indonesian', 633),\n",
       " ('slovenian', 628),\n",
       " ('portuguese', 612),\n",
       " ('people of the united states, american', 595),\n",
       " ('singaporean', 570),\n",
       " ('taiwanese', 558),\n",
       " ('burmese', 541),\n",
       " ('tanzanian', 539),\n",
       " ('nepali', 538),\n",
       " ('jpn', 534),\n",
       " ('icelandic', 518),\n",
       " ('filipino people, filipino', 515),\n",
       " ('canada', 511),\n",
       " ('germany', 497),\n",
       " ('malaysian', 491),\n",
       " ('france', 484),\n",
       " ('latvian', 483),\n",
       " ('soviet', 482),\n",
       " ('australia', 480),\n",
       " ('england, english', 474),\n",
       " ('belgium', 464),\n",
       " ('lithuanian', 459),\n",
       " ('(flagicon: usa) american', 457),\n",
       " ('puerto rican', 452),\n",
       " ('thai', 448),\n",
       " ('indian nationality, indian', 446)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_100 = Counter(nationality_df.nationality).most_common(100)\n",
    "#hidden tests are within this cell\n",
    "top_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "78fa1af4ba62f9e0204ce57adf0be060",
     "grade": false,
     "grade_id": "cell-26ee2d9d3d77f166",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Life Pro Tip: Always look at your data (always)\n",
    "As you might have noticed, there's a bit of noise in Wikipedia's labels! Sometimes nationality is reported as \"british\" and sometimes \"united kingdom, british\". This kind of weirdness is very common in real datasets. As a practitioner in NLP, you need to be on guard at all times against any kind of weirdness that might distort your results. A good habit to get into (starting with this lesson!) is to look at your data and see if you spot anything unusual. Sometimes (like in this lesson), you can easily clean up errors or inconsistencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c597d5764059d6f7ac5bddcf59d5e141",
     "grade": false,
     "grade_id": "cell-65acb554033fc8d1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 1.1.4: Fix the nationality labels\n",
    "We won't fix _everything_ but as a quick fix that should improve our data's nationality labels, you should use python's `split()` function to divide these labels when they have a comma and take the last word, which we'll treat as the official national label. *Important note:* Remember that `split` matches exactly what you put in, but there might be variable whitespace around the final token. Use `strip()` to ensure that no nationality has leading or trailing white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81fc1919db2291fdaa65bf1cd78f56fd",
     "grade": false,
     "grade_id": "cell-22a1e53953d9f61c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1.5: Double check the number of nationalities\n",
    "We've ideally cut down on the number of easy-to-fix nationality name issues, so let's check by printing the number of unique nationalities now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9fcd25937e54d8812788bed3112a5012",
     "grade": true,
     "grade_id": "num_nationalities_2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13616\n"
     ]
    }
   ],
   "source": [
    "print(len(set(nationality_df.nationality))) # Should be 7865\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4bf14604a68e87b169cc3e62a547c071",
     "grade": false,
     "grade_id": "cell-a62bab937cabebd4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "7,865 is still a big number, but we at least dropped a few thousand likely-bogus ones. For now, we'll move forward in the lesson, but recognize that when dealing with Real Data™ your work will likely involve lots of effort cleaning and sanitizing this data. I've intentionally started you out doing this practice to emphasize the need for double-checking and hopefully instill a healthy bit of skepticism (paranoia?) about data issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "078ef6438cedb68b428312c6f05f0e83",
     "grade": false,
     "grade_id": "cell-c9baa4bb0ed8a18a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 1.1.6: Print out the new list of 100 most common nationalities using Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ba811832bd29d0b66464bd120ef66fe",
     "grade": true,
     "grade_id": "most_common_nationalities_2",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('american', 43157),\n",
       " ('british', 16635),\n",
       " ('indian', 10761),\n",
       " ('united states, american', 9811),\n",
       " ('australian', 8285),\n",
       " ('french', 7365),\n",
       " ('german', 6628),\n",
       " ('italian', 5403),\n",
       " ('japanese', 5223),\n",
       " ('canadian', 5083),\n",
       " ('united states', 4915),\n",
       " ('americans, american', 3804),\n",
       " ('english', 3507),\n",
       " ('mexican', 3294),\n",
       " ('canadians, canadian', 3280),\n",
       " ('pakistani', 3114),\n",
       " ('spanish', 3090),\n",
       " ('usa', 2973),\n",
       " ('russian', 2910),\n",
       " ('united kingdom, british', 2894),\n",
       " ('norwegian', 2851),\n",
       " ('polish', 2790),\n",
       " ('chinese', 2456),\n",
       " ('irish', 2218),\n",
       " ('swedish', 2189),\n",
       " ('dutch', 2103),\n",
       " ('brazilian', 1936),\n",
       " ('austrian', 1812),\n",
       " ('south korean', 1762),\n",
       " ('nigerian', 1705),\n",
       " ('swiss', 1643),\n",
       " ('british people, british', 1621),\n",
       " ('bangladeshi', 1613),\n",
       " ('hungarian', 1599),\n",
       " ('belgian', 1597),\n",
       " ('germany, german', 1544),\n",
       " ('serbian', 1521),\n",
       " ('danish', 1454),\n",
       " ('turkish', 1433),\n",
       " ('south african', 1430),\n",
       " ('irish people, irish', 1418),\n",
       " ('romanian', 1389),\n",
       " ('france, french', 1376),\n",
       " ('argentine', 1362),\n",
       " ('eng', 1278),\n",
       " ('czech', 1262),\n",
       " ('scottish', 1159),\n",
       " ('indian people, indian', 1144),\n",
       " ('greek', 1142),\n",
       " ('finnish', 1129),\n",
       " ('sri lankan', 1101),\n",
       " ('filipino', 1070),\n",
       " ('italy, italian', 1068),\n",
       " ('aus', 1061),\n",
       " ('ghanaian', 1017),\n",
       " ('iranian', 1016),\n",
       " ('croatian', 1015),\n",
       " ('netherlands, dutch', 977),\n",
       " ('ugandan', 964),\n",
       " ('united kingdom', 954),\n",
       " ('egyptian', 942),\n",
       " ('bulgarian', 923),\n",
       " ('japanese people, japanese', 894),\n",
       " ('new zealand', 890),\n",
       " ('israeli', 855),\n",
       " ('india', 821),\n",
       " ('spain, spanish', 765),\n",
       " ('cuban', 765),\n",
       " ('ukrainian', 742),\n",
       " ('republic of china', 712),\n",
       " ('kenyan', 679),\n",
       " ('french people, french', 664),\n",
       " ('chilean', 637),\n",
       " ('english people, english', 633),\n",
       " ('indonesian', 633),\n",
       " ('slovenian', 628),\n",
       " ('portuguese', 612),\n",
       " ('people of the united states, american', 595),\n",
       " ('singaporean', 570),\n",
       " ('taiwanese', 558),\n",
       " ('burmese', 541),\n",
       " ('tanzanian', 539),\n",
       " ('nepali', 538),\n",
       " ('jpn', 534),\n",
       " ('icelandic', 518),\n",
       " ('filipino people, filipino', 515),\n",
       " ('canada', 511),\n",
       " ('germany', 497),\n",
       " ('malaysian', 491),\n",
       " ('france', 484),\n",
       " ('latvian', 483),\n",
       " ('soviet', 482),\n",
       " ('australia', 480),\n",
       " ('england, english', 474),\n",
       " ('belgium', 464),\n",
       " ('lithuanian', 459),\n",
       " ('(flagicon: usa) american', 457),\n",
       " ('puerto rican', 452),\n",
       " ('thai', 448),\n",
       " ('indian nationality, indian', 446)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_100 = Counter(nationality_df.nationality).most_common(100)\n",
    "#hidden tests are within this cell\n",
    "top_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1.7: Filter dataset to only those nationalities with at least 500 occurrences\n",
    "When training any classifier, you need enough examples to learn features that reliably predict the labels. For this homework, let's restrict ourselves to working with only nationalities that have at least 500 occurrences. Create a set called `final_nationalities` that contains only those with at least 500 occurrences. Then, from this restricted label set, let's take the subset of `nationality_df` that use these and make a new list called `cleaned_nationality_df` that holds our final dataset that we'll use for train, test, and development.\n",
    "\n",
    "*Side note:* Often, removing rare labels is another good way of getting rid of noise in our dataset. However, in practice, it's important to check these labels to make sure there are no (or few) systematic errors that would bias your model. Sometimes these biases can have significant real-world impact (e.g., underrepresenting people) and as an ethical data scientist, it's your job to combat the introduction of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2386ccc3450bf9ad3884148dd6e2f16d",
     "grade": false,
     "grade_id": "cell-0204af5cad7d71a6",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "89e46ce477f144a3c576ef6a61670d58",
     "grade": false,
     "grade_id": "cell-9ed47ee608b8a28c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 1.1.8: Print the number of nationalities with at least 500 occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d927c9a166ee9bfd83e2a7ee9a6e793",
     "grade": true,
     "grade_id": "num_nationalities_least_500",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_nationalities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mfinal_nationalities\u001b[49m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_nationalities' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(final_nationalities))\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much smaller!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "25e3a2b68f59d72e0f1ffd6bc21b7abe",
     "grade": false,
     "grade_id": "cell-4a0915938d3a6b52",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 1.1.9: Print the number of items in `cleaned_nationality_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58193b92b55a20a9e2f6f3a0d47e5072",
     "grade": true,
     "grade_id": "num_cleaned_items",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(len(cleaned_nationality_df))\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6fc2c31a287b5d28fdbe0999b813a1a",
     "grade": false,
     "grade_id": "cell-7960f7ef725dc690",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 1.1.10: Split dataset into test, train and dev\n",
    "We have a large enough dataset that we can effectively split it into train, development, and test sets, using the standard ratio of 80%, 10%, 10% for each, respectively. We'll use `split` from `numpy` to split the data into train, dev, and test separately. We'll call these `train_df`, `dev_df`, and `test_df`.  Note that `split` does not shuffle, so we'll use `DataFrame.sample()` and randomly resample our entire dataset to get a random shuffle before the split.\n",
    "\n",
    "*Important note*: Remember to set  `random_state` in `DataFrame.sample()` to our seed so that you end up with the same (random) ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0bbb04e8c59f29e6cff1c5248d2cc0f8",
     "grade": false,
     "grade_id": "cell-ac9d4ffa86a68982",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec74bb6150a49d221337df6414e326da",
     "grade": false,
     "grade_id": "cell-36f2b23065d4872f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 1.1.11: print the `bio` of the  first instance of your training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "773de37b84fcff1a079bd7cf8efb7bb1",
     "grade": true,
     "grade_id": "first_instance_train",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(train_df.iloc[0,:]['bio'])\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04ce3934ab6faba461d5f130a87b3835",
     "grade": false,
     "grade_id": "cell-c97dd7105b63aa25",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 1.1.12: Print the first instance of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e87a3877c523ec371b0a349e6e4dc49",
     "grade": true,
     "grade_id": "first_instance_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(test_df.iloc[0,:]['bio'])\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a09d81e1d78245933884d65dd00a27fc",
     "grade": false,
     "grade_id": "cell-234fbbb1ca323e8b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Final Sanity Checking / Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08a67d365ff4d24d53c0ee169a8d6aa2",
     "grade": false,
     "grade_id": "cell-3f9f43bd6ad04038",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 1.1.13: Check out the token frequencies\n",
    "We looked at the nationality labels, but what's all in the biographies? As a final sanity check, we'll take a look at this data to build a bit of intuition for steps that we'll take when training our classifier. As a first task, let's try tokenizing each biography and getting a count of all the unique words for biographies in `train_df` using `Counter`. The function Counter from collections creates a dict subclass for counting hashable objects. In this task we expect students to create 3 different “Counters”. For more details on Counter please visit the source: https://docs.python.org/3/library/collections.html\n",
    "\n",
    "\n",
    "What tokens should count as a \"word\" and how do we find them? For this exercise, we'll try extract three kinds of tokens using different methods to see what happens\n",
    "\n",
    "1. ws_tokens: dict count of tokens separated by whitespace\n",
    "2. alpha_ws_tokens: dict count of tokens separated by whitespace and are alpha numeric \n",
    "3. alpha_re_tokens: dict count of tokens separated by word boundaries that only consist of alphanumeric characters\n",
    "\n",
    "\n",
    "As quick example of how these are different, let's say we have the sentence \"My computer says 'I don't know...' but after thinking about it, I think it does.\"\n",
    "\n",
    "* The first case should return `['My', 'computer', 'says', \"'I\", \"don't\", \"know...'\", 'but', 'after', 'thinking', 'about', 'it,', 'I', 'think', 'it', 'does.']`, which we can see contains a bunch of tokens that have punctuation with them.\n",
    "\n",
    "* The second case should return `['My', 'computer', 'says', 'but', 'after', 'thinking', 'about', 'I', 'think', 'it']` which is filtering out a _lot_ more tokens. We see that any token with any punctuation gets removed. This is probably too much but the tokens do look cleaner\n",
    "\n",
    "* The third case should return `['My', 'computer', 'says', 'I', 'don', 't', 'know', 'but', 'after', 'thinking', 'about', 'it', 'I', 'think', 'it', 'does']`, which gives us all the tokens. Here we see that it's also split \"don't\" into two tokens too! We could modify our regex some to allow intra-token punctuation to avoid this but for now we'll keep it a it simple.\n",
    "\n",
    "To build some intuition, mentally estimate how many unique tokens you think will be in each set before starting the exercise. Will the third set be 80% of the size? 50%? \n",
    "\n",
    "_Hint:_ the default string `.split()` function can help\n",
    "\n",
    "_Hint:_ `re.fullmatch` and `re.match` do subtly different things but you only want to use one of them\n",
    "\n",
    "_Hint:_ the `re.findall` method may be useful here for one type of token\n",
    "\n",
    "_Hint:_ The tokens in case 2 are a subset of those in case 1. This means you can speed your token extraction a bit if you check smartly. \n",
    "\n",
    "_Hint:_ The reference implementation takes around 2 minutes and 40 seconds\n",
    "\n",
    "advise using regex to fill the variables above. For information on regex visit the source: https://www.rexegg.com/regex-quickstart.html\n",
    "Read and Explore the documentation: https://docs.python.org/3/library/re.htmlHighly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fef635b06a12f723ca8e9e5dbe04f44",
     "grade": false,
     "grade_id": "cell-50c88630cace3b38",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Fill this with any token (with anything in it!) for tokens separated by whitespace\n",
    "ws_tokens = Counter()\n",
    "\n",
    "# Fill this one with tokens separated by whitespace but constisting only of tokens\n",
    "# that are totally made of alphanumeric characters (you can use the \\w character\n",
    "# class in making the regex)\n",
    "alpha_ws_tokens = Counter()\n",
    "\n",
    "# Fill this one with the tokens separated by *word boundaries* (not white space) that consist\n",
    "# of alphanumeric characters (use \\w again)\n",
    "alpha_re_tokens = Counter()\n",
    "for bio in tqdm(train_df.bio):\n",
    "    #HINT: Iterate through all bios found in train_df:\n",
    "        #HINT: At the current bio, we may now split by whitespace to find words found at this instance. \n",
    "            #HINT: At the current word, given that word(s) are split base on whitespace, update our ws_tokens counter\n",
    "            \n",
    "            #HINT: If the current word is also alphanumeric: update our alpha_ws_tokens\n",
    "\t\t#HINT: Otherwise, we will ignore\n",
    "        \t#HINT: We may use built in regex functions to fill alpha_re_tokens by passing in the current bio\n",
    "             \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb852af7a4b274d5e6faf1eb20c9d9a7",
     "grade": false,
     "grade_id": "cell-df5963b8eb0ddcf0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 1.1.14: Print the sizes of each dictionary in order on separate line\n",
    "Note that the hidden tests here will check the sizes of the counters to make sure you've constructed your regexes correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5609e7788355963337ad5e17a5c1d644",
     "grade": true,
     "grade_id": "sizes_token_sets",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(len(ws_tokens))\n",
    "print(len(alpha_ws_tokens))\n",
    "print(len(alpha_re_tokens))\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a46dcf45cf8671a882d87b4b88b0c3d",
     "grade": false,
     "grade_id": "cell-4ddb114f28538f3b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Surprising! Why do you think there is such a difference in size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15ef3b21a096e30f06a6d60296941a6d",
     "grade": false,
     "grade_id": "cell-42fba0374f6a05b9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 1.1.15: Let's look at the most common 50 words in the third definition (alphanumeric tokens in a word boundary). Print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bcfc61e3ae5618906528d4c22b2733f6",
     "grade": true,
     "grade_id": "most_common_words",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "top_50 = alpha_re_tokens.most_common(50)\n",
    "#hidden tests are within this cell\n",
    "top_50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d118cdf2473b6b139db55c4910b801c9",
     "grade": false,
     "grade_id": "cell-aceb8e7742172bf2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 1.1.16: Plot the word distribution\n",
    "We certainly have a lot of unique words in our data! In many corpora, word frequencies follow [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law), which is a power-law like distribution. In essence, a few words are _very_ common and account for most of the tokens we have in the data (this is where the word-type/token distinction is important!), while many words are relatively rare and infrequently occur in our data. \n",
    "\n",
    "First, let's create two lists `x` and `y`, where `x` holds the word's rank when sorted by frequency (e.g., the most common word is `0`, the fifth most-common is `4`, etc.) and `y` holds the probability of the word at rank _i_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de08be7e690e94fe9c4ad3e0771e5db8",
     "grade": false,
     "grade_id": "cell-adb208c9d9019eaf",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see Zipf's law in action, let's plot the _probability_ of seeing a word on a log-scale y-axis and order our words by the most probable first and also log-scale the x-axis. Uncomment the lines below to plot this.\n",
    "\n",
    "_Hint:_ you can log-scale an axis in pyplot using `plt.yscale('log')` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = plt.plot(x, y, '.')\n",
    "# plt.yscale('log')\n",
    "# plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, would you look at that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing your classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23bfbf51a0a7bcb8af53808551718c60",
     "grade": false,
     "grade_id": "cell-39a34867332e728e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 1.1.17: Convert your text data to features\n",
    "The dataset has been prepared and the time has now arrived to actually start doing some predictions! We'll be using a `TfIdfVectorizer` to convert the text into features. There are several important things to note:\n",
    "\n",
    "1. We have a *lot* of words. There are almost too many to feasibly use unless we're running on a powerful computer. _But_ as we saw above, most words are actually relatively rare. This rarity is quite useful for us because it means we can remove these words as features to our classifier and they shouldn't affect performance too much (after all, the classifier can't learn from features that are rarely present).\n",
    "2. In addition to rare words, there are generally a few very common words that appear in most comments. These are often known as _stop words_ like \"the\". In most settings (but not all!), these features don't add much information so we can safely remove them to be more efficient.\n",
    "\n",
    "The [TfIdfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) class thankfully provides easy ways for us to do both. We'll use `min_df` to ensure that word show up at least 500 times and use `stop_words` to specify their default `english` list. \n",
    "\n",
    "Create this `TfIdfVectorizer` and call it vectorizer. Then, call `fit_transform` on the list of biographies in  `train_df` to convert the text into a matrix of features we'll call `X_train`. `X` is the standard name you'll see for feature matrices in machine learning, and usually it has a suffix in code to indicate which data it came from, e.g., `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3ffdd277d3df4a93db123af6da51b76",
     "grade": false,
     "grade_id": "cell-4e7ca2b4b4916639",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1f308e4675025ce600d5de4e5756015",
     "grade": false,
     "grade_id": "cell-fa32e21ebab08fc3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 1.1.18: Sanity Check: print the shape of X_train\n",
    "Let's ensure that we featurized everything as expected. You should have 6,009 word features in your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a75edb6e4c7abf6f5cbdd02b8e4ee981",
     "grade": true,
     "grade_id": "shape_x_train",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1.19: Get the list of labels\n",
    "We need to get the final list of labels in a python `list` for sklearn to use. Create this list from `train_df` and let's call it `y_train`. `y` (lower case!) is normally used to refer to the label of the classifier (or value in  a regressor) in machine learning. We use the lower case here to indicate it's a vector, whereas `X` is upper case because it's a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = list(train_df.nationality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1.20: Fit the classifier on a subset of the data\n",
    "Finally, let's fit the classifier. For a start we'll use `LogisticRegression`. Don't forget to set the `random_state` to use our `RANDOM_SEED` so you get deterministic (but random) results. To train your classifier, create a `LogisticRegression` object (typically classifiers are named `clf`) and call `fit` passing in `X_train` and `y_train`.\n",
    "\n",
    "For this cell, let's just use the first 10,000 rows of `X_train` and `y_train` to fit the classifier. In general, when you have a large dataset, it's useful to go end-to-end and train one of these half-baked classifiers to verify that your model works as expected. You can even do some analyses if the performance is good enough to get a sense of how things are working. Then you can train on the full data.\n",
    "\n",
    "*Notes:*\n",
    "1. You should make sure to use the `lbfgs` solver, as this generally Just Works™ and is fast.\n",
    "2. Since we have more than two nationalities, we'll set `multi_class='auto'` so that the classifier isn't binary.\n",
    "3. `X_train` is a numpy array, so you'll need to use array indexing operations to get the first 10,000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87d5125846e5197bc2a900af00ceea25",
     "grade": false,
     "grade_id": "cell-4488f53817039cfd",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1af046f47116f023fef543a7943d6ee1",
     "grade": false,
     "grade_id": "cell-19f567d667525d41",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 1.1.21: Generate dev data\n",
    "Let's generate the numpy matrices for the development data. Take the text in our `dev_df` and pass it through the vectorizer to turn it into features. We'll call this `X_dev`. Also create a list of the corresponding labels for each item, which we'll call `y_dev`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d5b05c3c4a7b72ec688ddc583a2261a",
     "grade": false,
     "grade_id": "cell-9de8b8929774c01a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3c2c8d20e18a969ecc69961d0edf42f",
     "grade": false,
     "grade_id": "cell-15dfa5b32182ec7d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 1.1.22: Create Dummy classifiers\n",
    "It's always important to contextualize your results by comparing it with naive classifiers. If these classifiers do well, then your task is easy! If not, then you can see how much better your system does at first. We'll use two different strategies using the [Dummy Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html) class. Create two `DummyClassifier` instances that use the `uniform` (guess randomly) and `most_frequent` strategies and fit these on the training data so we can compare them with our regressor that was trained on 10K instances. In general, you probably always want to at least compare with these two baselines in a classification task.\n",
    "\n",
    "*NOTE:* Be sure to set the `random_state` of the `DummyClassifier` to be `RANDOM_SEED` so your scores match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0250a401f8b078cd165e6dc438907cc3",
     "grade": false,
     "grade_id": "cell-4e9f756a83492d0e",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1.23: Generate all the predictions\n",
    "Let's generate our predictions. We have three models: our `LogisticRegression` model trained on 10K items and two `DummyClassifier` models that are baselines. Using our `X_dev` data, predict the nationality for each person and store these as:\n",
    "* `lr_tiny_dev_preds`\n",
    "* `rand_dev_preds`\n",
    "* `mf_dev_preds`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e325f9c91e9634b5330e5c4683548307",
     "grade": false,
     "grade_id": "cell-1993587c73d4bd2c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88108c06ce96bfed5d489183fb0cf680",
     "grade": false,
     "grade_id": "cell-9ba283e93b6ed6bc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 1.1.24: Score our predictions\n",
    "Now, let's score the models. Here, we'll use F1 to score and use a _macro_ average so that the score reflects the average F1 performance across all classes. Many NLP problems have more than two labels and we care about our performance on each. The macro-averaged F1 is especially important in these multiclass settings where some labels are less common, since it will tell us how well we're doing overall. \n",
    "\n",
    "Score your three models on the dev set using macro-averaged F1 and save these scores as `lr_f1`, `rand_f1`, and `mf_f1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_f1 = f1_score(y_dev, lr_tiny_dev_preds, average='macro')\n",
    "rand_f1 = f1_score(y_dev, rand_dev_preds, average='macro')\n",
    "mf_f1 = f1_score(y_dev, mf_dev_preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49b1637a597aa2b6cd24db0f17bc4849",
     "grade": true,
     "grade_id": "f1_scores_partial",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(lr_f1)\n",
    "print(rand_f1)\n",
    "print(mf_f1)\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, pretty good even for training on just 10K items! But still lots of room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3d21e0fa23d442e9e6b2f48917f67a5",
     "grade": false,
     "grade_id": "cell-b445e1578a568787",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 1.1.25: Fit the classifier on the full data\n",
    "Let's see if we can improve our performance with more data. Train a new `LogisticRegression` model on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d803d27c8272e41681396e11be99fd4",
     "grade": false,
     "grade_id": "cell-52b5233e83f8c78f",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61912030b70b27b873af3fec1e0ea09b",
     "grade": false,
     "grade_id": "cell-254b4756fb9ab908",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 1.1.26: Generate all the predictions for the model and score it\n",
    "Save your predictions as `lr_dev_preds` and the model's performance as `lr_f1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_dev_preds = clf.predict(X_dev)\n",
    "lr_f1 = f1_score(y_dev, lr_dev_preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a1c2eb3204a3cc79606b355b6ac7201",
     "grade": true,
     "grade_id": "f1_scores_full",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(lr_f1)\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why might this performance be so high? Hint: Think about some of the most common words you might see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ffaaa18f64008945fd13bd9e2f430eb3",
     "grade": false,
     "grade_id": "cell-4ffd369aeccdd5eb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# How much data do we need?\n",
    "With performance so high, how much data do we need to get an accurate classifier? For NLP, it's often useful to see how the performance changes relative to how many training examples you have. For some tasks with heavily structured text, you might only need a few hundred examples to get good performance&mdash;but for others with highly variable text, you might need tens of thousands to learn generalizable features across all of the data. \n",
    "\n",
    "In this part of the exercise, we'll re-use parts of our code to generate performance numbers for random samples of the full dataset.\n",
    "\n",
    "Using part of your code above, finishing the function below that receives two data frames to use as training and evaluation. The function should fit a new `TfidfVectorizer` and train a LogisticRegression classifier from the training data and then evaluate on the provided dev data. The function returns the macro-averaged F1 score on the dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b965e2e9b1ee5e85ea4143a9a60cb73b",
     "grade": false,
     "grade_id": "cell-a4e3284a01db7de8",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def train_and_score(train_df, dev_df):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on different subsets of the data to see how performance increases with dataset size\n",
    "Using our `train_and_score` function, we'll test how the same classification model changes in performance as we add more data. In the code below, write a function called `change_in_performance`. You will use your output to answer the prompts in the Exercise 1.1 Quiz in Coursera.\n",
    "\n",
    "**Before you submit your assignment to the autograde, be sure to comment out the function call.**\n",
    "\n",
    "_NOTE 1:_ If this method is slow at first, try using a smaller sample of `dev_items` when initially debugging and then use the full `dev_items` after\n",
    "\n",
    "_NOTE 2:_ For speed, we're only recording one performance number here. However if you expect to see a lot of variability in your models, it's worth evaluating multiple models for each training set size and reporting bootstrapped F1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "237b6e7e2640add09f57efdf8837c3e8",
     "grade": false,
     "grade_id": "cell-cabb62465a0932dc",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def change_in_performance(training_sizes, train_df, dev_df):\n",
    "    random.seed(RANDOM_SEED)\n",
    "    f1_scores = []\n",
    "    for training_size in tqdm(training_sizes):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    return(f1_scores)\n",
    "\n",
    "training_sizes = [1000, 10000, 50000, 100000]\n",
    "# performace_f1_scores = change_in_performance(training_sizes, train_df, dev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the performances for each training data set size\n",
    "We'll use seaborn's barplot to show performance\n",
    "\n",
    "**Be sure to comment out the code below before you submit to the autograder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(performace_f1_scores)\n",
    "\n",
    "# df = pd.DataFrame({'num_instances': training_sizes, 'f1': performace_f1_scores})\n",
    "# sns.barplot(data=df, x='num_instances', y='f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall lots of room for improvement. We could certainly try fine-tuning some of the hyperparameters though! Some useful ideas to try by altering the TF-IDF vectorizer or classifier:\n",
    "* use lower `min_df` to increase the number of features\n",
    "* don't use stopword removal\n",
    "* tune the `C` parameter on the logistic regression classifier\n",
    "* don't lower-case the text \n",
    "* use a `CountVectorizer` instead of a TF-IDF vectorizer\n",
    "* set `max_df` to remove common features\n",
    "\n",
    "Which do you think will lead to higher performance? Try some out and report on the Slack or Piazza what's the highest performance you can achieve!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sequences as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1.27: Fit a unigram and bigram LogisticRegression classifier\n",
    "Unigrams and Bigrams can be powerful features  for classification. Let's see if our model gets better performance if we train a new model that now includes bigrams.\n",
    "\n",
    "Create a new `TfidfVectorizer` with the same hyperparameter values but include a specification for `ngram_range` to use both unigrams and bigrams. Then call `fit_transform` on the training data to create a new feature matrix `X_train` with these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram_vectorizer = TfidfVectorizer(stop_words='english', min_df=500, ngram_range=(1,2))\n",
    "# X_train = bigram_vectorizer.fit_transform(train_df.bio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1.28: Print the feature matrix shape when using unigrams and bigrams\n",
    "Before you run this, it's useful to think about how many features you had before with unigrams. How many new bigrams do you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5a2d1272bf5479a82b0d70ef80db318",
     "grade": true,
     "grade_id": "cell-d580ab5b2b1a3bd8",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1.29: Train the unigram and bigram classifier\n",
    "Create a new `LogisticRegression` classifier model and fit it on the `X_train` and `y_train` data. Note that we don't have to recreate `y_train` since we are only changing how we featurize the text (not the labels  associated with the text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf461942de408f9d42ec32ad148800d8",
     "grade": false,
     "grade_id": "cell-f4e457d1d11cc031",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurize the development data\n",
    "Use your new unigram+bigram featurizer to featurize the dev data and call this `X_dev`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4cceeb4e15e4a76adcfd82af943805ab",
     "grade": false,
     "grade_id": "cell-40b399d467836148",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the new model to generate dev predictions and score them\n",
    "Using your newly-trained model, generate predictions from it and score them using macro-average F1. Save the output in a variable called `lr_f1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0e7d9d94a2f45f3e2aa0b0a861446d1",
     "grade": false,
     "grade_id": "cell-bd3399802c89013b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3e2a8723be70c66c0cad390a27c7e24",
     "grade": true,
     "grade_id": "cell-53cb9c3769a10fe1",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# print(lr_f1)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_applied_natural_language_processing_v2_assignment1_part1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
